version: 2.1

commands:
    destroy-environment:
        description: Destroy back-end and front-end cloudformation stacks given a workflow ID.
        parameters:
          workflow_id:
             type: string 
             default: "${CIRCLE_WORKFLOW_ID:0:7}"
        steps:
          - run:
              name: Destroy environments
              when: on_fail
              # will pass in specific workflow ID on_destroy in jobs or default
              # remove s3 objects and/or delete cloudformation stacks on_fail
              command: |
                echo "Destroying environment: << parameters.workflow_id >> "
                aws s3 rm s3://udapeople-<<parameters.workflow_id>> --recursive
                aws cloudformation delete-stack --stack-name udapeople-backend-<<parameters.workflow_id>>
                aws cloudformation delete-stack --stack-name udapeople-frontend-<<parameters.workflow_id>>
  
    revert-migrations:
      description: Revert the last migration if successfully run in the current workflow.
      parameters:
        # Add parameter here
        workflow_id:
           type: string
           default: "${CIRCLE_WORKFLOW_ID}"
      steps:
        - run:
            name: Revert migrations
            when: on_fail
            command: |
              SUCCESS=$(curl --insecure https://kvdb.io/2syWY5Xs2KEx1idTNW29LB/migration_${CIRCLE_WORKFLOW_ID:0:7})
              if (($SUCCESS == 1))
              then
                cd ~/project/backend
                npm install
                npm run build
                npm run migrations:revert
              fi

jobs:
    build-frontend:
        docker:
          - image: circleci/node:13.8.0
        steps:
          - checkout
          # Restore from cache
          - restore_cache:
              keys: [frontend-build]
          - run:
              name: Build front-end
              command: |
                cd frontend
                npm install
                npm run build
          - save_cache:
              paths: [frontend/node_modules]
              key: frontend-build
    build-backend:
        docker:
          - image: circleci/node:13.8.0
        steps:
          - checkout
          # Restore from cache
          - restore_cache:
              keys: [backend-build]
          - run:
              name: Back-end build
              command: |
                 cd backend
                 npm install
                 npm run build
          - save_cache:
              paths: [backend/node_modules]
              key: backend-build
    test-frontend:
        docker:
          - image: circleci/node:13.8.0
        steps:
          - checkout
          - restore_cache:
              keys: [frontend-build]
          - run:
             name: Test run frontend
             command: |
               cd frontend
               npm install
               npm run test
                
    test-backend:
        docker:
          - image: circleci/node:13.8.0
        steps:
          - checkout
          - restore_cache:
              keys: [backend-build]
          - run:
             name: Run backend test
             command: |
                cd backend
                npm install
                npm run test

    scan-frontend:
      docker:
        - image: circleci/node:13.8.0
      steps:
        - checkout
        - restore_cache:
            keys: [frontend-build]
        - run:
            name: Scan Frontend
            command: |
              cd frontend
              npm install
              npm audit fix --force --audit-level=critical
              npm audit fix --force --audit-level=critical
              npm audit --audit-level=critical

    scan-backend:
      docker:
        - image: circleci/node:13.8.0
      steps:
        - checkout
        - restore_cache:
            keys: [backend-build]
        - run:
            name: Scan Backend Dependencies
            command: |
              cd backend
              npm install
              npm audit fix --force --audit-level=critical
              npm audit fix --force --audit-level=critical
              npm audit --audit-level=critical
    
    deploy-infrastructure:
        docker:
          # Docker image with AWS CLI support
          - image: amazon/aws-cli
        steps:
          - checkout
          # install tar, gzip
          - run: yum install tar gzip -y
          - run:
              name: Ensure back-end infrastructure exists
              command: |
                aws cloudformation deploy \
                  --template-file .circleci/files/backend.yml \
                  --tags project=udapeople \
                  --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
                  --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"  
          - run:
              name: Ensure front-end infrastructure exist
              command: |
                aws cloudformation deploy \
                  --template-file .circleci/files/frontend.yml \
                  --tags project=udapeople \
                  --stack-name "udapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7}" \
                  --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"
          # Get IP of EC2 instance server from AWS EC2, append to inventory.txt file
          - run:
              name: Add back-end ip to ansible inventory
              command: |
                aws ec2 describe-instances \
                --query "Reservations[*].Instances[*].[PublicIpAddress]" \
                --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
                --output text >> ~/project/.circleci/ansible/inventory.txt
                cat ~/project/.circleci/ansible/inventory.txt
          - persist_to_workspace:
              root: ~/
              paths:
                - project/.circleci/ansible/inventory.txt
          # Rollback on on_fail
          - destroy-environment:
              workflow_id: ${CIRCLE_WORKFLOW_ID:0:7} 
    
    configure-infrastructure:
      docker:
        # Docker image here that supports Ansible
        - image: python:3.10-alpine3.16
      steps:
        - checkout
        # Add ssh keys with fingerprint
        - add_ssh_keys:
           fingerprints:
              - "58:8a:d0:31:65:2f:3e:99:4a:c2:a2:bc:ef:ce:73:39"
        # attach workspace
        - attach_workspace:
            at: ~/
        - run:
            name: Install ansible, tar gzip aws-cli openssh
            command: |
              apk add --update --no-cache ansible tar gzip aws-cli openssh
        - run:
            name: Configure server
            command: |
              cd .circleci/ansible
              ls -a
              ansible-playbook -i inventory.txt configure-server.yml
            no_output_timeout: 35m
        # Rollback on failure
        - destroy-environment
    
    run-migrations:
      docker:
        # Docker image here that supports NodeJS
        - image: circleci/node:13.8.0
      steps:
        # Checkout code from git
        - checkout
        # - restore_cache:
        #     keys: [backend-build]
        - run:
            name: Run migrations
            command: |
              cd backend
              npm install --no-audit --no-fund
              # Run and save the migration output
              npm run migrations > migrations_dump.txt
              # print migration txt to stdout
              cat migrations_dump.txt
        - run:
            name: Send migration data to kvdb.io bucket
            command: |
              if grep -q "has been executed successfully." ~/project/backend/migrations_dump.txt
              then
                  # from kvdb.io, generate the bucket ID "9GE4jRtKznmVKRfvdBABBe" in your local terminal
                  curl --insecure https://kvdb.io/2syWY5Xs2KEx1idTNW29LB/migration_${CIRCLE_WORKFLOW_ID:0:7} -d '1'
              fi
       # Here's where: rollback on failure
        - revert-migrations
  
    deploy-frontend:
      docker:
         # Docker image here that supports AWS CLI
        - image: python:3.10-alpine3.16
      steps:
        - checkout
        # persist to workspace
        - attach_workspace:
            at: ~/
        - run:
            name: Setup gzip tar curl aws-cli npm nodejs
            command: |
              apk add --update --no-cache tar gzip
              apk add --update --no-cache nodejs npm
              apk add --update --no-cache aws-cli curl
        - run:
            name: Get Backend URL
            command: |
              export BACKEND_IP=$(aws ec2 describe-instances \
              --query "Reservations[*].Instances[*].[PublicIpAddress]" \
              --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --output text)
              export API_URL="http://${BACKEND_IP}:3030"
              echo "API_URL=${API_URL}"
              # reddirect API url to .env file 
              echo API_URL="http://${BACKEND_IP}:3030" > frontend/.env
              # print to stdout
              cat frontend/.env
        - run:
            name: Deploy frontend objects
            command: |
              cd frontend
              npm install
              npm run build
              tar -czvf artifact-"${CIRCLE_WORKFLOW_ID:0:7}".tar.gz dist
              aws s3 cp dist s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7} --recursive 
              
        # Here's where you will add some code to rollback on failure
        - destroy-environment
        - revert-migrations
    
    deploy-backend:
      docker:
        # Docker image here that supports Ansible
        - image: python:3.10-alpine3.16
      steps:
        # checkout code from git
        - checkout
        # ddd ssh key fingerprint
        - add_ssh_keys:
           fingerprints:
              - "58:8a:d0:31:65:2f:3e:99:4a:c2:a2:bc:ef:ce:73:39"
        # attach workspace
        - attach_workspace:
            at: ~/
        - run:
            name: Install dependencies
            command: |
              apk add --update --no-cache ansible
              apk add --update --no-cache tar gzip
              apk add --update --no-cache nodejs npm
              apk add --update --no-cache aws-cli curl
        - run:
            name: Deploy Backend
            command: |
              cd backend
              
              # set up environment variables for backend connections
              touch .env
              echo ENVIRONMENT=production > ".env"
              echo TYPEORM_CONNECTION=postgres >> ".env"
              echo TYPEORM_ENTITIES=./src/modules/domain/**/*.entity.ts >> ".env"
              echo TYPEORM_MIGRATIONS=./src/migrations/*.ts >> ".env"
              echo TYPEORM_MIGRATIONS_DIR=./src/migrations >> ".env"
              echo NODE_ENV=production >> ".env"
              echo TYPEORM_HOST=$TYPEORM_HOST >> ".env"
              echo TYPEORM_PORT=$TYPEORM_PORT >> ".env"
              echo TYPEORM_USERNAME=$TYPEORM_USERNAME >> ".env"
              echo TYPEORM_PASSWORD=$TYPEORM_PASSWORD >> ".env"
              echo TYPEORM_DATABASE=$TYPEORM_DATABASE >> ".env"

              npm install
              npm run build
              cd ..
              pwd
              ls -a
              # zip the backend directory
              tar -C backend -czvf artifact.tar.gz .

              # create directory in deploy roles && move zip artifact to deploy/role for ansible
              mkdir .circleci/ansible/roles/deploy/files/
              mv artifact.tar.gz .circleci/ansible/roles/deploy/files/artifact.tar.gz

              # navigate to ansible & display content of inventory
              cd .circleci/ansible
              echo "Contents  of the inventory.txt file is -------"
              cat inventory.txt

              ansible-playbook -i inventory.txt deploy-backend.yml
        # Here's where you will add some code to rollback on failure
        - destroy-environment
        - revert-migrations

    smoke-test:
      docker:
        # Docker image that support job tasks
        - image: python:3.10-alpine3.16 
      steps:
        # Checkout code from git
        - checkout
        - run:
            name: Install dependencies
            command: |
              apk add --update --no-cache tar gzip
              apk add --update --no-cache curl aws-cli
              apk add --update --no-cache nodejs npm
        - run:
            name: Backend smoke test
            command: |
              BACKEND_PUB_IP=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --query "Reservations[*].Instances[*].[PublicIpAddress]" \
              --output text)

              export API_URL=http://${BACKEND_PUB_IP}:3030
              echo "API_URL=${API_URL}"

              echo "Wait for Server to accept GET connection request:"
              sleep 60
              if [[ 'ok' =~ $(curl -s --connect-timeout 5 ${API_URL}/api/status | grep -o ok) ]];then return 0 else return 1;fi

        - run:
            name: Frontend smoke test
            command: |
              URL=http://udapeople-${CIRCLE_WORKFLOW_ID:0:7}.s3-website-${AWS_DEFAULT_REGION}.amazonaws.com/#/employees
              echo ${URL}
              
              if curl -s ${URL} | grep "Welcome"
              then
                # Change this(from 1) to 0 after the job fails
                return 0
              else
                return 1
              fi
              
        - destroy-environment
        - revert-migrations
        
    cloudfront-update:
      docker:
        # Docker image here that supports AWS CLI
        - image: amazon/aws-cli
      steps:
        - checkout
        - run:
            name: Install dependencies
            command: |
              yum install tar gzip -y
        - run:
            name: Update cloudfront distribution
            command: |
              export OldWorkflowID=$(aws cloudformation list-exports \
              --query "Exports[?Name==\`WorkflowID\`].Value" \
              --no-paginate --output text)

              echo $OldWorkflowID >> ~/project/OldWorkflowID.txt
              cat ~/project/OldWorkflowID.txt
            
              # list out old and current workflow IDs
              echo "${OldWorkflowID}"
              echo CurrentWorflowID: "${CIRCLE_WORKFLOW_ID:0:7}"
              
              # update CDN
              aws cloudformation deploy \
              --template-file .circleci/files/cloudfront.yml \
              --stack-name InitialStack \
              --parameter-overrides WorkflowID="${CIRCLE_WORKFLOW_ID:0:7}" \
              --tags project=udapeople
        # persist workspace
        - persist_to_workspace:
            root: ~/
            paths:
              - project/OldWorkflowID.txt

        - destroy-environment
        - revert-migrations
        
    cleanup:
      docker:
        # Docker image with AWS CLI support
        - image: amazon/aws-cli
      steps:
        # check out from git
        - checkout
        - run:
            name: Install dependencies
            command: |
              yum install tar gzip -y
        # attach workspace '..oldworkflowID.txt data'
        - attach_workspace:
            at: ~/
        - run:
            name: Fetch OldStack workflow ID
            command: |
              export STACKS=($(aws cloudformation list-stacks --query "StackSummaries[*].StackName" \
              --stack-status-filter CREATE_COMPLETE --no-paginate --output text))
              echo Stack names: "${STACKS[@]}"
              cat ~/project/OldWorkflowID.txt
              export OldWorkflowID=$(cat ~/project/OldWorkflowID.txt)
              echo OldWorkflowID: "${OldWorkflowID}"
        - run:
            name: Cleanup OldStacks
            command: |
              export OldWorkflowID=$(cat ~/project/OldWorkflowID.txt)
              if [[ "${CIRCLE_WORKFLOW_ID:0:7}" != "${OldWorkflowID}" ]]
              then
                echo "----------Deletes cofirmed------------"
                echo "udapeople-${OldWorkflowID}"
                aws s3 rm "s3://udapeople-${OldWorkflowID}" --recursive || true
                aws cloudformation delete-stack --stack-name "udapeople-backend-${OldWorkflowID}" || true
                aws cloudformation delete-stack --stack-name "udapeople-frontend-${OldWorkflowID}" || true
              else
                echo  "----------------Cannot cleanup------------"
              fi

workflows:
    default:
        jobs:
          - build-frontend
          - build-backend
          - test-frontend:
                requires: [build-frontend]
          - test-backend:
                requires: [build-backend]
          - scan-backend:
              requires: [build-backend]
          - scan-frontend:
              requires: [build-frontend]
          - deploy-infrastructure:
              requires: [test-frontend, test-backend, scan-frontend, scan-backend]
              filters:
                branches:
                  only: [master]
          - configure-infrastructure:
              requires: [deploy-infrastructure]
          - run-migrations:
              requires: [configure-infrastructure]
          - deploy-frontend:
              requires: [run-migrations]
          - deploy-backend:
              requires: [run-migrations]
          - smoke-test:
              requires: [deploy-backend, deploy-frontend]
          - cloudfront-update:
              requires: [smoke-test]
          - cleanup:
              requires: [cloudfront-update]